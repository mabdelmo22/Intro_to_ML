{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aIWHL3uHySVx",
        "outputId": "cfe23a93-f2af-4e12-a811-ba9ee2e680c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Stochastic Gradient Descent (SGD):\n",
            "\n",
            "Learning rate: 0.1\n",
            "\n",
            "Epoch 500, Training loss nan, Validation loss nan\n",
            "Epoch 1000, Training loss nan, Validation loss nan\n",
            "Epoch 1500, Training loss nan, Validation loss nan\n",
            "Epoch 2000, Training loss nan, Validation loss nan\n",
            "Epoch 2500, Training loss nan, Validation loss nan\n",
            "Epoch 3000, Training loss nan, Validation loss nan\n",
            "Epoch 3500, Training loss nan, Validation loss nan\n",
            "Epoch 4000, Training loss nan, Validation loss nan\n",
            "Epoch 4500, Training loss nan, Validation loss nan\n",
            "Epoch 5000, Training loss nan, Validation loss nan\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epoch 500, Training loss nan, Validation loss nan\n",
            "Epoch 1000, Training loss nan, Validation loss nan\n",
            "Epoch 1500, Training loss nan, Validation loss nan\n",
            "Epoch 2000, Training loss nan, Validation loss nan\n",
            "Epoch 2500, Training loss nan, Validation loss nan\n",
            "Epoch 3000, Training loss nan, Validation loss nan\n",
            "Epoch 3500, Training loss nan, Validation loss nan\n",
            "Epoch 4000, Training loss nan, Validation loss nan\n",
            "Epoch 4500, Training loss nan, Validation loss nan\n",
            "Epoch 5000, Training loss nan, Validation loss nan\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epoch 500, Training loss nan, Validation loss nan\n",
            "Epoch 1000, Training loss nan, Validation loss nan\n",
            "Epoch 1500, Training loss nan, Validation loss nan\n",
            "Epoch 2000, Training loss nan, Validation loss nan\n",
            "Epoch 2500, Training loss nan, Validation loss nan\n",
            "Epoch 3000, Training loss nan, Validation loss nan\n",
            "Epoch 3500, Training loss nan, Validation loss nan\n",
            "Epoch 4000, Training loss nan, Validation loss nan\n",
            "Epoch 4500, Training loss nan, Validation loss nan\n",
            "Epoch 5000, Training loss nan, Validation loss nan\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epoch 500, Training loss 11.9179, Validation loss 4.7955\n",
            "Epoch 1000, Training loss 9.5623, Validation loss 3.3341\n",
            "Epoch 1500, Training loss 7.9316, Validation loss 2.3322\n",
            "Epoch 2000, Training loss 6.8021, Validation loss 1.6464\n",
            "Epoch 2500, Training loss 6.0192, Validation loss 1.1780\n",
            "Epoch 3000, Training loss 5.4761, Validation loss 0.8588\n",
            "Epoch 3500, Training loss 5.0987, Validation loss 0.6419\n",
            "Epoch 4000, Training loss 4.8360, Validation loss 0.4951\n",
            "Epoch 4500, Training loss 4.6525, Validation loss 0.3961\n",
            "Epoch 5000, Training loss 4.5239, Validation loss 0.3298\n",
            "Adam Optimizer:\n",
            "\n",
            "Learning rate: 0.1\n",
            "\n",
            "Epoch 500, Training loss 3.2683, Validation loss 0.3116\n",
            "Epoch 1000, Training loss 2.8387, Validation loss 0.4661\n",
            "Epoch 1500, Training loss 2.5311, Validation loss 0.7121\n",
            "Epoch 2000, Training loss 2.3825, Validation loss 0.9766\n",
            "Epoch 2500, Training loss 2.3346, Validation loss 1.1795\n",
            "Epoch 3000, Training loss 2.3250, Validation loss 1.2913\n",
            "Epoch 3500, Training loss 2.3239, Validation loss 1.3347\n",
            "Epoch 4000, Training loss 2.3238, Validation loss 1.3459\n",
            "Epoch 4500, Training loss 2.3238, Validation loss 1.3477\n",
            "Epoch 5000, Training loss 2.3238, Validation loss 1.3478\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epoch 500, Training loss 6.8923, Validation loss 1.9439\n",
            "Epoch 1000, Training loss 4.4719, Validation loss 0.7446\n",
            "Epoch 1500, Training loss 3.6559, Validation loss 0.3503\n",
            "Epoch 2000, Training loss 3.4782, Validation loss 0.2861\n",
            "Epoch 2500, Training loss 3.3991, Validation loss 0.2878\n",
            "Epoch 3000, Training loss 3.3128, Validation loss 0.3028\n",
            "Epoch 3500, Training loss 3.2112, Validation loss 0.3265\n",
            "Epoch 4000, Training loss 3.0951, Validation loss 0.3601\n",
            "Epoch 4500, Training loss 2.9676, Validation loss 0.4069\n",
            "Epoch 5000, Training loss 2.8341, Validation loss 0.4704\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epoch 500, Training loss 105.8463, Validation loss 99.5980\n",
            "Epoch 1000, Training loss 14.0143, Validation loss 12.7216\n",
            "Epoch 1500, Training loss 9.2036, Validation loss 3.7497\n",
            "Epoch 2000, Training loss 8.7666, Validation loss 2.9025\n",
            "Epoch 2500, Training loss 8.2934, Validation loss 2.6419\n",
            "Epoch 3000, Training loss 7.7418, Validation loss 2.3676\n",
            "Epoch 3500, Training loss 7.1271, Validation loss 2.0615\n",
            "Epoch 4000, Training loss 6.4751, Validation loss 1.7359\n",
            "Epoch 4500, Training loss 5.8212, Validation loss 1.4078\n",
            "Epoch 5000, Training loss 5.2065, Validation loss 1.0978\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epoch 500, Training loss 598.8035, Validation loss 485.8737\n",
            "Epoch 1000, Training loss 507.8668, Validation loss 416.7753\n",
            "Epoch 1500, Training loss 427.1293, Validation loss 354.9579\n",
            "Epoch 2000, Training loss 355.6421, Validation loss 299.7432\n",
            "Epoch 2500, Training loss 292.6307, Validation loss 250.5799\n",
            "Epoch 3000, Training loss 237.4580, Validation loss 207.0179\n",
            "Epoch 3500, Training loss 189.5900, Validation loss 168.6844\n",
            "Epoch 4000, Training loss 148.5623, Validation loss 135.2617\n",
            "Epoch 4500, Training loss 113.9500, Validation loss 106.4654\n",
            "Epoch 5000, Training loss 85.3403, Validation loss 82.0272\n",
            "Epoch 500, Training loss 3.2683, Validation loss 0.3116\n",
            "Epoch 1000, Training loss 2.8387, Validation loss 0.4661\n",
            "Epoch 1500, Training loss 2.5311, Validation loss 0.7121\n",
            "Epoch 2000, Training loss 2.3825, Validation loss 0.9766\n",
            "Epoch 2500, Training loss 2.3346, Validation loss 1.1795\n",
            "Epoch 3000, Training loss 2.3250, Validation loss 1.2913\n",
            "Epoch 3500, Training loss 2.3239, Validation loss 1.3347\n",
            "Epoch 4000, Training loss 2.3238, Validation loss 1.3459\n",
            "Epoch 4500, Training loss 2.3238, Validation loss 1.3477\n",
            "Epoch 5000, Training loss 2.3238, Validation loss 1.3478\n",
            "Epoch 1, Training loss 93.9443, Validation loss 19.2545\n",
            "Epoch 2, Training loss 92.9896, Validation loss 18.7192\n",
            "Epoch 3, Training loss 92.0433, Validation loss 18.1919\n",
            "Epoch 500, Training loss 28.3364, Validation loss 8.2007\n",
            "Epoch 1000, Training loss 17.2090, Validation loss 6.8235\n",
            "Epoch 1500, Training loss 9.9365, Validation loss 5.5731\n",
            "Epoch 2000, Training loss 5.8570, Validation loss 4.6397\n",
            "Epoch 2500, Training loss 3.9281, Validation loss 4.0133\n",
            "Epoch 3000, Training loss 3.2047, Validation loss 3.6341\n",
            "Epoch 3500, Training loss 3.0077, Validation loss 3.4326\n",
            "Epoch 4000, Training loss 2.9729, Validation loss 3.3434\n",
            "Epoch 4500, Training loss 2.9693, Validation loss 3.3130\n",
            "Epoch 5000, Training loss 2.9692, Validation loss 3.3057\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "#Problem 1\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Temperature data\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "\n",
        "# Convert lists to tensors\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "# Extract training and validation data\n",
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "# Normalize the training and validation inputs\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u\n",
        "\n",
        "# Define non-linear model\n",
        "def nonlinear_model(t_u, w1, w2, b):\n",
        "    return w2 * t_u ** 2 + w1 * t_u + b\n",
        "\n",
        "# Define linear model\n",
        "def linear_model(t_u, w, b):\n",
        "    return w * t_u + b\n",
        "\n",
        "# Define loss function\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c) ** 2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "# Training loop for non-linear model\n",
        "def nonlinear_train(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = nonlinear_model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "        val_t_p = nonlinear_model(val_t_u, *params)\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f}, Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "# Training loop for linear model\n",
        "def linear_train(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = linear_model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "        val_t_p = linear_model(val_t_u, *params)\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f}, Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "# Experiment with different learning rates and optimizers\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "print('Stochastic Gradient Descent (SGD):\\n')\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "    optimizer = optim.SGD([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    nonlinear_train(n_epochs=5000, optimizer=optimizer, params=params, train_t_u=train_t_un, train_t_c=train_t_c, val_t_u=val_t_un, val_t_c=val_t_c)\n",
        "\n",
        "print('Adam Optimizer:\\n')\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "    optimizer = optim.Adam([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    nonlinear_train(n_epochs=5000, optimizer=optimizer, params=params, train_t_u=train_t_un, train_t_c=train_t_c, val_t_u=val_t_un, val_t_c=val_t_c)\n",
        "\n",
        "# Compare the best non-linear model with a linear model\n",
        "nonlinear_params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "nonlinear_optimizer = optim.Adam([nonlinear_params], lr=0.1)\n",
        "linear_params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "linear_optimizer = optim.Adam([linear_params], lr=0.01)\n",
        "\n",
        "nonlinear_params = nonlinear_train(n_epochs=5000, optimizer=nonlinear_optimizer, params=nonlinear_params, train_t_u=train_t_un, train_t_c=train_t_c, val_t_u=val_t_un, val_t_c=val_t_c)\n",
        "linear_params = linear_train(n_epochs=5000, optimizer=linear_optimizer, params=linear_params, train_t_u=train_t_un, train_t_c=train_t_c, val_t_u=val_t_un, val_t_c=val_t_c)\n",
        "\n",
        "t_p_linear = linear_model(t_u, *linear_params).detach().numpy()\n",
        "t_p_nonlinear = nonlinear_model(t_u, *nonlinear_params).detach().numpy()\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(t_u, t_c, label='Measurements')\n",
        "plt.plot(t_u, t_p_linear, label='Linear Model')\n",
        "plt.plot(t_u, t_p_nonlinear, label='Nonlinear Model')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Loading the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/mabdelmo22/Intro_to_ML/main/Housing.csv'\n",
        "housing = pd.read_csv(file_path)\n",
        "\n",
        "# Displaying the first 10 rows of the dataset\n",
        "print(housing.head(10))\n",
        "\n",
        "# Function for 6-parameter linear model\n",
        "def linear_model1(x, params):\n",
        "    return torch.matmul(x, params[:-1]) + params[-1]\n",
        "\n",
        "# Function for 11-parameter linear model\n",
        "def linear_model2(x, params):\n",
        "    return torch.matmul(x, params[:-1]) + params[-1]\n",
        "\n",
        "# Training function for 6-parameter model\n",
        "def linear_train1(epochs, optimizer, params, train_x, val_x, train_y, val_y, loss_fn):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_p = linear_model1(train_x, params)\n",
        "        train_loss = loss_fn(train_p, train_y)\n",
        "        val_p = linear_model1(val_x, params)\n",
        "        val_loss = loss_fn(val_p, val_y)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epochs: {epoch}, \\tTraining Loss: {train_loss:.6f}, \\tValidation Loss: {val_loss:.6f}')\n",
        "    return params, train_losses, val_losses\n",
        "\n",
        "# Training function for 11-parameter model\n",
        "def linear_train2(epochs, optimizer, params, train_x, val_x, train_y, val_y, loss_fn):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_p = linear_model2(train_x, params)\n",
        "        train_loss = loss_fn(train_p, train_y)\n",
        "        val_p = linear_model2(val_x, params)\n",
        "        val_loss = loss_fn(val_p, val_y)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epochs: {epoch}, \\tTraining Loss: {train_loss:.6f}, \\tValidation Loss: {val_loss:.6f}')\n",
        "    return params, train_losses, val_losses\n",
        "\n",
        "# Selecting features and target variable for the 6-parameter model\n",
        "features = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]\n",
        "x = housing[features].values\n",
        "y = housing['price'].values\n",
        "\n",
        "# Standardizing the features\n",
        "sc = StandardScaler()\n",
        "x_sc = sc.fit_transform(x)\n",
        "\n",
        "# Converting to tensors\n",
        "x = torch.tensor(x_sc, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Splitting into training and validation sets\n",
        "n_samples = x.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "train_x = x[train_indices]\n",
        "train_y = y[train_indices]\n",
        "val_x = x[val_indices]\n",
        "val_y = y[val_indices]\n",
        "\n",
        "# Defining the loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Training the 6-parameter model with different learning rates using Adam optimizer\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0] * 5 + [0.0], dtype=torch.float32, requires_grad=True)\n",
        "    optimizer = optim.Adam([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    trained_params, train_losses, val_losses = linear_train1(5000, optimizer, params, train_x, val_x, train_y, val_y, loss_fn)\n",
        "\n",
        "# Training the 6-parameter model with different learning rates using SGD optimizer\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0] * 5 + [0.0], dtype=torch.float32, requires_grad=True)\n",
        "    optimizer = optim.SGD([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    trained_params, train_losses, val_losses = linear_train1(5000, optimizer, params, train_x, val_x, train_y, val_y, loss_fn)\n",
        "\n",
        "# Encoding binary categorical variables\n",
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning']\n",
        "binary_map = lambda x: x.map({'yes': 1, 'no': 0})\n",
        "housing[varlist] = housing[varlist].apply(binary_map)\n",
        "print(housing.head())\n",
        "\n",
        "# Selecting features and target variable for the 11-parameter model\n",
        "feature_list = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking']\n",
        "x1 = housing[feature_list].values\n",
        "y1 = housing['price'].values.reshape(-1, 1)\n",
        "\n",
        "# Standardizing the features\n",
        "x1_sc = sc.fit_transform(x1)\n",
        "\n",
        "# Converting to tensors\n",
        "x1 = torch.tensor(x1_sc, dtype=torch.float32)\n",
        "y1 = torch.tensor(y1, dtype=torch.float32)\n",
        "\n",
        "# Splitting into training and validation sets\n",
        "n_samples = x1.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "train_x1 = x1[train_indices]\n",
        "train_y1 = y1[train_indices]\n",
        "val_x1 = x1[val_indices]\n",
        "val_y1 = y1[val_indices]\n",
        "\n",
        "# Training the 11-parameter model with different learning rates using Adam optimizer\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0] * 10 + [0.0], dtype=torch.float32, requires_grad=True)\n",
        "    optimizer = optim.Adam([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    trained_params, train_losses, val_losses = linear_train2(5000, optimizer, params, train_x1, val_x1, train_y1, val_y1, loss_fn)\n",
        "\n",
        "# Training the 11-parameter model with different learning rates using SGD optimizer\n",
        "for lr in learning_rates:\n",
        "    params = torch.tensor([1.0] * 10 + [0.0], dtype=torch.float32, requires_grad=True)\n",
        "    optimizer = optim.SGD([params], lr=lr)\n",
        "    print(f'Learning rate: {lr}\\n')\n",
        "    trained_params, train_losses, val_losses = linear_train2(5000, optimizer, params, train_x1, val_x1, train_y1, val_y1, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drKGlQJQ015U",
        "outputId": "b94f55ee-8f86-40cf-cd49-7103b0e55e93"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      price   area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
            "0  13300000   7420         4          2        3      yes        no       no   \n",
            "1  12250000   8960         4          4        4      yes        no       no   \n",
            "2  12250000   9960         3          2        2      yes        no      yes   \n",
            "3  12215000   7500         4          2        2      yes        no      yes   \n",
            "4  11410000   7420         4          1        2      yes       yes      yes   \n",
            "5  10850000   7500         3          3        1      yes        no      yes   \n",
            "6  10150000   8580         4          3        4      yes        no       no   \n",
            "7  10150000  16200         5          3        2      yes        no       no   \n",
            "8   9870000   8100         4          1        2      yes       yes      yes   \n",
            "9   9800000   5750         3          2        4      yes       yes       no   \n",
            "\n",
            "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
            "0              no             yes        2      yes        furnished  \n",
            "1              no             yes        3       no        furnished  \n",
            "2              no              no        2      yes   semi-furnished  \n",
            "3              no             yes        3      yes        furnished  \n",
            "4              no             yes        2       no        furnished  \n",
            "5              no             yes        2      yes   semi-furnished  \n",
            "6              no             yes        2      yes   semi-furnished  \n",
            "7              no              no        0       no      unfurnished  \n",
            "8              no             yes        2      yes        furnished  \n",
            "9              no             yes        1      yes      unfurnished  \n",
            "Learning rate: 0.1\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26520917639168.000000, \tValidation Loss: 24980985217024.000000\n",
            "Epochs: 1000, \tTraining Loss: 26520009572352.000000, \tValidation Loss: 24980161036288.000000\n",
            "Epochs: 1500, \tTraining Loss: 26519103602688.000000, \tValidation Loss: 24979336855552.000000\n",
            "Epochs: 2000, \tTraining Loss: 26518193438720.000000, \tValidation Loss: 24978512674816.000000\n",
            "Epochs: 2500, \tTraining Loss: 26517285371904.000000, \tValidation Loss: 24977684299776.000000\n",
            "Epochs: 3000, \tTraining Loss: 26516375207936.000000, \tValidation Loss: 24976862216192.000000\n",
            "Epochs: 3500, \tTraining Loss: 26515462946816.000000, \tValidation Loss: 24976038035456.000000\n",
            "Epochs: 4000, \tTraining Loss: 26514559074304.000000, \tValidation Loss: 24975211757568.000000\n",
            "Epochs: 4500, \tTraining Loss: 26513651007488.000000, \tValidation Loss: 24974387576832.000000\n",
            "Epochs: 5000, \tTraining Loss: 26512738746368.000000, \tValidation Loss: 24973563396096.000000\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26521735528448.000000, \tValidation Loss: 24981729705984.000000\n",
            "Epochs: 1000, \tTraining Loss: 26521645350912.000000, \tValidation Loss: 24981647917056.000000\n",
            "Epochs: 1500, \tTraining Loss: 26521555173376.000000, \tValidation Loss: 24981564030976.000000\n",
            "Epochs: 2000, \tTraining Loss: 26521464995840.000000, \tValidation Loss: 24981480144896.000000\n",
            "Epochs: 2500, \tTraining Loss: 26521374818304.000000, \tValidation Loss: 24981400453120.000000\n",
            "Epochs: 3000, \tTraining Loss: 26521280446464.000000, \tValidation Loss: 24981314469888.000000\n",
            "Epochs: 3500, \tTraining Loss: 26521192366080.000000, \tValidation Loss: 24981234778112.000000\n",
            "Epochs: 4000, \tTraining Loss: 26521100091392.000000, \tValidation Loss: 24981150892032.000000\n",
            "Epochs: 4500, \tTraining Loss: 26521009913856.000000, \tValidation Loss: 24981067005952.000000\n",
            "Epochs: 5000, \tTraining Loss: 26520917639168.000000, \tValidation Loss: 24980983119872.000000\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26521819414528.000000, \tValidation Loss: 24981801009152.000000\n",
            "Epochs: 1000, \tTraining Loss: 26521808928768.000000, \tValidation Loss: 24981792620544.000000\n",
            "Epochs: 1500, \tTraining Loss: 26521800540160.000000, \tValidation Loss: 24981786329088.000000\n",
            "Epochs: 2000, \tTraining Loss: 26521790054400.000000, \tValidation Loss: 24981777940480.000000\n",
            "Epochs: 2500, \tTraining Loss: 26521779568640.000000, \tValidation Loss: 24981771649024.000000\n",
            "Epochs: 3000, \tTraining Loss: 26521773277184.000000, \tValidation Loss: 24981761163264.000000\n",
            "Epochs: 3500, \tTraining Loss: 26521762791424.000000, \tValidation Loss: 24981752774656.000000\n",
            "Epochs: 4000, \tTraining Loss: 26521758597120.000000, \tValidation Loss: 24981746483200.000000\n",
            "Epochs: 4500, \tTraining Loss: 26521746014208.000000, \tValidation Loss: 24981733900288.000000\n",
            "Epochs: 5000, \tTraining Loss: 26521735528448.000000, \tValidation Loss: 24981729705984.000000\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26521827803136.000000, \tValidation Loss: 24981809397760.000000\n",
            "Epochs: 1000, \tTraining Loss: 26521823608832.000000, \tValidation Loss: 24981809397760.000000\n",
            "Epochs: 1500, \tTraining Loss: 26521823608832.000000, \tValidation Loss: 24981809397760.000000\n",
            "Epochs: 2000, \tTraining Loss: 26521823608832.000000, \tValidation Loss: 24981807300608.000000\n",
            "Epochs: 2500, \tTraining Loss: 26521823608832.000000, \tValidation Loss: 24981805203456.000000\n",
            "Epochs: 3000, \tTraining Loss: 26521821511680.000000, \tValidation Loss: 24981805203456.000000\n",
            "Epochs: 3500, \tTraining Loss: 26521819414528.000000, \tValidation Loss: 24981805203456.000000\n",
            "Epochs: 4000, \tTraining Loss: 26521819414528.000000, \tValidation Loss: 24981805203456.000000\n",
            "Epochs: 4500, \tTraining Loss: 26521819414528.000000, \tValidation Loss: 24981801009152.000000\n",
            "Epochs: 5000, \tTraining Loss: 26521819414528.000000, \tValidation Loss: 24981801009152.000000\n",
            "Learning rate: 0.1\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 1000, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 1500, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 2000, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 2500, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 3000, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 3500, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 4000, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 4500, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Epochs: 5000, \tTraining Loss: 1526647488512.000000, \tValidation Loss: 1568188792832.000000\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 1526648143872.000000, \tValidation Loss: 1568142786560.000000\n",
            "Epochs: 1000, \tTraining Loss: 1526647357440.000000, \tValidation Loss: 1568187351040.000000\n",
            "Epochs: 1500, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 2000, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 2500, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 3000, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 3500, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 4000, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 4500, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Epochs: 5000, \tTraining Loss: 1526647750656.000000, \tValidation Loss: 1568187482112.000000\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 4694811344896.000000, \tValidation Loss: 4473531400192.000000\n",
            "Epochs: 1000, \tTraining Loss: 1954241183744.000000, \tValidation Loss: 1921890910208.000000\n",
            "Epochs: 1500, \tTraining Loss: 1586305302528.000000, \tValidation Loss: 1601149337600.000000\n",
            "Epochs: 2000, \tTraining Loss: 1535375966208.000000, \tValidation Loss: 1566564679680.000000\n",
            "Epochs: 2500, \tTraining Loss: 1528026628096.000000, \tValidation Loss: 1565512826880.000000\n",
            "Epochs: 3000, \tTraining Loss: 1526890627072.000000, \tValidation Loss: 1566836785152.000000\n",
            "Epochs: 3500, \tTraining Loss: 1526696378368.000000, \tValidation Loss: 1567601197056.000000\n",
            "Epochs: 4000, \tTraining Loss: 1526658498560.000000, \tValidation Loss: 1567939100672.000000\n",
            "Epochs: 4500, \tTraining Loss: 1526650372096.000000, \tValidation Loss: 1568080134144.000000\n",
            "Epochs: 5000, \tTraining Loss: 1526648274944.000000, \tValidation Loss: 1568137805824.000000\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 21722518519808.000000, \tValidation Loss: 20486956253184.000000\n",
            "Epochs: 1000, \tTraining Loss: 17874349457408.000000, \tValidation Loss: 16875477008384.000000\n",
            "Epochs: 1500, \tTraining Loss: 14784505315328.000000, \tValidation Loss: 13971166855168.000000\n",
            "Epochs: 2000, \tTraining Loss: 12295847542784.000000, \tValidation Loss: 11629372637184.000000\n",
            "Epochs: 2500, \tTraining Loss: 10286130003968.000000, \tValidation Loss: 9736959295488.000000\n",
            "Epochs: 3000, \tTraining Loss: 8659568427008.000000, \tValidation Loss: 8204871794688.000000\n",
            "Epochs: 3500, \tTraining Loss: 7340643319808.000000, \tValidation Loss: 6962622234624.000000\n",
            "Epochs: 4000, \tTraining Loss: 6269479419904.000000, \tValidation Loss: 5954128576512.000000\n",
            "Epochs: 4500, \tTraining Loss: 5398374907904.000000, \tValidation Loss: 5134587789312.000000\n",
            "Epochs: 5000, \tTraining Loss: 4689188356096.000000, \tValidation Loss: 4468088766464.000000\n",
            "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
            "0  13300000  7420         4          2        3         1          0   \n",
            "1  12250000  8960         4          4        4         1          0   \n",
            "2  12250000  9960         3          2        2         1          0   \n",
            "3  12215000  7500         4          2        2         1          0   \n",
            "4  11410000  7420         4          1        2         1          1   \n",
            "\n",
            "   basement  hotwaterheating  airconditioning  parking prefarea  \\\n",
            "0         0                0                1        2      yes   \n",
            "1         0                0                1        3       no   \n",
            "2         1                0                0        2      yes   \n",
            "3         1                0                1        3      yes   \n",
            "4         1                0                1        2       no   \n",
            "\n",
            "  furnishingstatus  \n",
            "0        furnished  \n",
            "1        furnished  \n",
            "2   semi-furnished  \n",
            "3        furnished  \n",
            "4        furnished  \n",
            "Learning rate: 0.1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([109])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 500, \tTraining Loss: 26201999540224.000000, \tValidation Loss: 26258792513536.000000\n",
            "Epochs: 1000, \tTraining Loss: 26201424920576.000000, \tValidation Loss: 26258717016064.000000\n",
            "Epochs: 1500, \tTraining Loss: 26200848203776.000000, \tValidation Loss: 26258645712896.000000\n",
            "Epochs: 2000, \tTraining Loss: 26200275681280.000000, \tValidation Loss: 26258570215424.000000\n",
            "Epochs: 2500, \tTraining Loss: 26199701061632.000000, \tValidation Loss: 26258494717952.000000\n",
            "Epochs: 3000, \tTraining Loss: 26199126441984.000000, \tValidation Loss: 26258419220480.000000\n",
            "Epochs: 3500, \tTraining Loss: 26198553919488.000000, \tValidation Loss: 26258345820160.000000\n",
            "Epochs: 4000, \tTraining Loss: 26197977202688.000000, \tValidation Loss: 26258266128384.000000\n",
            "Epochs: 4500, \tTraining Loss: 26197404680192.000000, \tValidation Loss: 26258194825216.000000\n",
            "Epochs: 5000, \tTraining Loss: 26196830060544.000000, \tValidation Loss: 26258121424896.000000\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26202517536768.000000, \tValidation Loss: 26258861719552.000000\n",
            "Epochs: 1000, \tTraining Loss: 26202456719360.000000, \tValidation Loss: 26258853330944.000000\n",
            "Epochs: 1500, \tTraining Loss: 26202402193408.000000, \tValidation Loss: 26258844942336.000000\n",
            "Epochs: 2000, \tTraining Loss: 26202347667456.000000, \tValidation Loss: 26258838650880.000000\n",
            "Epochs: 2500, \tTraining Loss: 26202286850048.000000, \tValidation Loss: 26258830262272.000000\n",
            "Epochs: 3000, \tTraining Loss: 26202228129792.000000, \tValidation Loss: 26258821873664.000000\n",
            "Epochs: 3500, \tTraining Loss: 26202173603840.000000, \tValidation Loss: 26258815582208.000000\n",
            "Epochs: 4000, \tTraining Loss: 26202112786432.000000, \tValidation Loss: 26258811387904.000000\n",
            "Epochs: 4500, \tTraining Loss: 26202056163328.000000, \tValidation Loss: 26258798804992.000000\n",
            "Epochs: 5000, \tTraining Loss: 26201999540224.000000, \tValidation Loss: 26258792513536.000000\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26202567868416.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 1000, \tTraining Loss: 26202561576960.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 1500, \tTraining Loss: 26202555285504.000000, \tValidation Loss: 26258865913856.000000\n",
            "Epochs: 2000, \tTraining Loss: 26202553188352.000000, \tValidation Loss: 26258865913856.000000\n",
            "Epochs: 2500, \tTraining Loss: 26202544799744.000000, \tValidation Loss: 26258865913856.000000\n",
            "Epochs: 3000, \tTraining Loss: 26202538508288.000000, \tValidation Loss: 26258861719552.000000\n",
            "Epochs: 3500, \tTraining Loss: 26202534313984.000000, \tValidation Loss: 26258861719552.000000\n",
            "Epochs: 4000, \tTraining Loss: 26202530119680.000000, \tValidation Loss: 26258859622400.000000\n",
            "Epochs: 4500, \tTraining Loss: 26202523828224.000000, \tValidation Loss: 26258859622400.000000\n",
            "Epochs: 5000, \tTraining Loss: 26202517536768.000000, \tValidation Loss: 26258861719552.000000\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 26202576257024.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 1000, \tTraining Loss: 26202576257024.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 1500, \tTraining Loss: 26202576257024.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 2000, \tTraining Loss: 26202576257024.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 2500, \tTraining Loss: 26202569965568.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 3000, \tTraining Loss: 26202569965568.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 3500, \tTraining Loss: 26202567868416.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 4000, \tTraining Loss: 26202567868416.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 4500, \tTraining Loss: 26202567868416.000000, \tValidation Loss: 26258868011008.000000\n",
            "Epochs: 5000, \tTraining Loss: 26202567868416.000000, \tValidation Loss: 26258868011008.000000\n",
            "Learning rate: 0.1\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 1000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 1500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 2000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 2500, \tTraining Loss: 3682564571136.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 3000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 3500, \tTraining Loss: 3682564571136.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 4000, \tTraining Loss: 3682564571136.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 4500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Epochs: 5000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732614746112.000000\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 3682567979008.000000, \tValidation Loss: 2732769673216.000000\n",
            "Epochs: 1000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617891840.000000\n",
            "Epochs: 1500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 2000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 2500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 3000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 3500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 4000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 4500, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Epochs: 5000, \tTraining Loss: 3682564833280.000000, \tValidation Loss: 2732617105408.000000\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 6735553626112.000000, \tValidation Loss: 6344384446464.000000\n",
            "Epochs: 1000, \tTraining Loss: 4104005091328.000000, \tValidation Loss: 3349806317568.000000\n",
            "Epochs: 1500, \tTraining Loss: 3742471028736.000000, \tValidation Loss: 2859642126336.000000\n",
            "Epochs: 2000, \tTraining Loss: 3691534614528.000000, \tValidation Loss: 2765696532480.000000\n",
            "Epochs: 2500, \tTraining Loss: 3684049616896.000000, \tValidation Loss: 2743229743104.000000\n",
            "Epochs: 3000, \tTraining Loss: 3682857648128.000000, \tValidation Loss: 2736540876800.000000\n",
            "Epochs: 3500, \tTraining Loss: 3682637447168.000000, \tValidation Loss: 2734207533056.000000\n",
            "Epochs: 4000, \tTraining Loss: 3682587639808.000000, \tValidation Loss: 2733308903424.000000\n",
            "Epochs: 4500, \tTraining Loss: 3682573221888.000000, \tValidation Loss: 2732939542528.000000\n",
            "Epochs: 5000, \tTraining Loss: 3682567979008.000000, \tValidation Loss: 2732778061824.000000\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epochs: 500, \tTraining Loss: 22108384002048.000000, \tValidation Loss: 22177365622784.000000\n",
            "Epochs: 1000, \tTraining Loss: 18756027809792.000000, \tValidation Loss: 18807907155968.000000\n",
            "Epochs: 1500, \tTraining Loss: 16016430071808.000000, \tValidation Loss: 16031136350208.000000\n",
            "Epochs: 2000, \tTraining Loss: 13777014620160.000000, \tValidation Loss: 13741720600576.000000\n",
            "Epochs: 2500, \tTraining Loss: 11946024763392.000000, \tValidation Loss: 11853228933120.000000\n",
            "Epochs: 3000, \tTraining Loss: 10448604758016.000000, \tValidation Loss: 10294640246784.000000\n",
            "Epochs: 3500, \tTraining Loss: 9223712800768.000000, \tValidation Loss: 9007657910272.000000\n",
            "Epochs: 4000, \tTraining Loss: 8221533143040.000000, \tValidation Loss: 7944365670400.000000\n",
            "Epochs: 4500, \tTraining Loss: 7401399386112.000000, \tValidation Loss: 7065371148288.000000\n",
            "Epochs: 5000, \tTraining Loss: 6730096836608.000000, \tValidation Loss: 6338278064128.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "#Problem 3\n",
        "# Load dataset\n",
        "url = 'https://raw.githubusercontent.com/mabdelmo22/Intro_to_ML/main/Housing.csv'\n",
        "housing_data = pd.read_csv(url)\n",
        "print(housing_data.head(10))\n",
        "\n",
        "# Linear models\n",
        "def linear_model_6(x, params):\n",
        "    return torch.matmul(x, params[:-1]) + params[-1]\n",
        "\n",
        "def linear_model_11(x, params):\n",
        "    return torch.matmul(x, params[:-1]) + params[-1]\n",
        "\n",
        "# Training functions\n",
        "def train_linear_model_6(epochs, optimizer, params, train_x, val_x, train_y, val_y, loss_function):\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        train_pred = linear_model_6(train_x, params)\n",
        "        train_loss = loss_function(train_pred, train_y)\n",
        "        val_pred = linear_model_6(val_x, params)\n",
        "        val_loss = loss_function(val_pred, val_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if (epoch + 1) % 500 == 0:\n",
        "            print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "    return params, train_losses, val_losses\n",
        "\n",
        "def train_linear_model_11(epochs, optimizer, params, train_x, val_x, train_y, val_y, loss_function):\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        train_pred = linear_model_11(train_x, params)\n",
        "        train_loss = loss_function(train_pred, train_y)\n",
        "        val_pred = linear_model_11(val_x, params)\n",
        "        val_loss = loss_function(val_pred, val_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if (epoch + 1) % 500 == 0:\n",
        "            print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "    return params, train_losses, val_losses\n",
        "\n",
        "# Data preparation for 6-parameter model\n",
        "features_6 = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]\n",
        "x_6 = housing_data[features_6].values\n",
        "y_6 = housing_data['price'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_6_scaled = scaler.fit_transform(x_6)\n",
        "\n",
        "x_6_tensor = torch.tensor(x_6_scaled, dtype=torch.float32)\n",
        "y_6_tensor = torch.tensor(y_6, dtype=torch.float32)\n",
        "\n",
        "num_samples = x_6_tensor.shape[0]\n",
        "num_val = int(0.2 * num_samples)\n",
        "indices = torch.randperm(num_samples)\n",
        "train_indices_6 = indices[:-num_val]\n",
        "val_indices_6 = indices[-num_val:]\n",
        "\n",
        "train_x_6 = x_6_tensor[train_indices_6]\n",
        "train_y_6 = y_6_tensor[train_indices_6]\n",
        "val_x_6 = x_6_tensor[val_indices_6]\n",
        "val_y_6 = y_6_tensor[val_indices_6]\n",
        "\n",
        "# Training the 6-parameter model with Adam optimizer\n",
        "loss_function = torch.nn.MSELoss()\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rates:\n",
        "    params_6 = torch.tensor([1.0] * 5 + [0.0], requires_grad=True)\n",
        "    optimizer_6 = optim.Adam([params_6], lr=lr)\n",
        "    print(f'Adam Optimizer - Learning Rate: {lr}')\n",
        "    trained_params_6, train_losses_6, val_losses_6 = train_linear_model_6(\n",
        "        5000, optimizer_6, params_6, train_x_6, val_x_6, train_y_6, val_y_6, loss_function)\n",
        "\n",
        "# Training the 6-parameter model with SGD optimizer\n",
        "for lr in learning_rates:\n",
        "    params_6 = torch.tensor([1.0] * 5 + [0.0], requires_grad=True)\n",
        "    optimizer_6 = optim.SGD([params_6], lr=lr)\n",
        "    print(f'SGD Optimizer - Learning Rate: {lr}')\n",
        "    trained_params_6, train_losses_6, val_losses_6 = train_linear_model_6(\n",
        "        5000, optimizer_6, params_6, train_x_6, val_x_6, train_y_6, val_y_6, loss_function)\n",
        "\n",
        "# Encode binary variables and prepare data for 11-parameter model\n",
        "binary_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning']\n",
        "housing_data[binary_columns] = housing_data[binary_columns].apply(lambda x: x.map({'yes': 1, 'no': 0}))\n",
        "print(housing_data.head())\n",
        "\n",
        "features_11 = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking']\n",
        "x_11 = housing_data[features_11].values\n",
        "y_11 = housing_data['price'].values.reshape(-1, 1)\n",
        "\n",
        "x_11_scaled = scaler.fit_transform(x_11)\n",
        "\n",
        "x_11_tensor = torch.tensor(x_11_scaled, dtype=torch.float32)\n",
        "y_11_tensor = torch.tensor(y_11, dtype=torch.float32)\n",
        "\n",
        "train_indices_11 = indices[:-num_val]\n",
        "val_indices_11 = indices[-num_val:]\n",
        "\n",
        "train_x_11 = x_11_tensor[train_indices_11]\n",
        "train_y_11 = y_11_tensor[train_indices_11]\n",
        "val_x_11 = x_11_tensor[val_indices_11]\n",
        "val_y_11 = y_11_tensor[val_indices_11]\n",
        "\n",
        "# Training the 11-parameter model with Adam optimizer\n",
        "for lr in learning_rates:\n",
        "    params_11 = torch.tensor([1.0] * 10 + [0.0], requires_grad=True)\n",
        "    optimizer_11 = optim.Adam([params_11], lr=lr)\n",
        "    print(f'Adam Optimizer - Learning Rate: {lr}')\n",
        "    trained_params_11, train_losses_11, val_losses_11 = train_linear_model_11(\n",
        "        5000, optimizer_11, params_11, train_x_11, val_x_11, train_y_11, val_y_11, loss_function)\n",
        "\n",
        "# Training the 11-parameter model with SGD optimizer\n",
        "for lr in learning_rates:\n",
        "    params_11 = torch.tensor([1.0] * 10 + [0.0], requires_grad=True)\n",
        "    optimizer_11 = optim.SGD([params_11], lr=lr)\n",
        "    print(f'SGD Optimizer - Learning Rate: {lr}')\n",
        "    trained_params_11, train_losses_11, val_losses_11 = train_linear_model_11(\n",
        "        5000, optimizer_11, params_11, train_x_11, val_x_11, train_y_11, val_y_11, loss_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ETnht224dHS",
        "outputId": "067abc5f-accb-4e9b-fa84-735f589b666f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      price   area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
            "0  13300000   7420         4          2        3      yes        no       no   \n",
            "1  12250000   8960         4          4        4      yes        no       no   \n",
            "2  12250000   9960         3          2        2      yes        no      yes   \n",
            "3  12215000   7500         4          2        2      yes        no      yes   \n",
            "4  11410000   7420         4          1        2      yes       yes      yes   \n",
            "5  10850000   7500         3          3        1      yes        no      yes   \n",
            "6  10150000   8580         4          3        4      yes        no       no   \n",
            "7  10150000  16200         5          3        2      yes        no       no   \n",
            "8   9870000   8100         4          1        2      yes       yes      yes   \n",
            "9   9800000   5750         3          2        4      yes       yes       no   \n",
            "\n",
            "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
            "0              no             yes        2      yes        furnished  \n",
            "1              no             yes        3       no        furnished  \n",
            "2              no              no        2      yes   semi-furnished  \n",
            "3              no             yes        3      yes        furnished  \n",
            "4              no             yes        2       no        furnished  \n",
            "5              no             yes        2      yes   semi-furnished  \n",
            "6              no             yes        2      yes   semi-furnished  \n",
            "7              no              no        0       no      unfurnished  \n",
            "8              no             yes        2      yes        furnished  \n",
            "9              no             yes        1      yes      unfurnished  \n",
            "Adam Optimizer - Learning Rate: 0.1\n",
            "Epoch: 500, Train Loss: 26795124457472.000000, Val Loss: 23884174721024.000000\n",
            "Epoch: 1000, Train Loss: 26794193321984.000000, Val Loss: 23883436523520.000000\n",
            "Epoch: 1500, Train Loss: 26793262186496.000000, Val Loss: 23882698326016.000000\n",
            "Epoch: 2000, Train Loss: 26792328953856.000000, Val Loss: 23881962225664.000000\n",
            "Epoch: 2500, Train Loss: 26791397818368.000000, Val Loss: 23881228222464.000000\n",
            "Epoch: 3000, Train Loss: 26790468780032.000000, Val Loss: 23880485830656.000000\n",
            "Epoch: 3500, Train Loss: 26789537644544.000000, Val Loss: 23879751827456.000000\n",
            "Epoch: 4000, Train Loss: 26788606509056.000000, Val Loss: 23879013629952.000000\n",
            "Epoch: 4500, Train Loss: 26787677470720.000000, Val Loss: 23878277529600.000000\n",
            "Epoch: 5000, Train Loss: 26786742140928.000000, Val Loss: 23877541429248.000000\n",
            "Adam Optimizer - Learning Rate: 0.01\n",
            "Epoch: 500, Train Loss: 26795961221120.000000, Val Loss: 23884835323904.000000\n",
            "Epoch: 1000, Train Loss: 26795866849280.000000, Val Loss: 23884761923584.000000\n",
            "Epoch: 1500, Train Loss: 26795774574592.000000, Val Loss: 23884690620416.000000\n",
            "Epoch: 2000, Train Loss: 26795678105600.000000, Val Loss: 23884615122944.000000\n",
            "Epoch: 2500, Train Loss: 26795590025216.000000, Val Loss: 23884539625472.000000\n",
            "Epoch: 3000, Train Loss: 26795495653376.000000, Val Loss: 23884466225152.000000\n",
            "Epoch: 3500, Train Loss: 26795401281536.000000, Val Loss: 23884392824832.000000\n",
            "Epoch: 4000, Train Loss: 26795309006848.000000, Val Loss: 23884317327360.000000\n",
            "Epoch: 4500, Train Loss: 26795214635008.000000, Val Loss: 23884246024192.000000\n",
            "Epoch: 5000, Train Loss: 26795118166016.000000, Val Loss: 23884172623872.000000\n",
            "Adam Optimizer - Learning Rate: 0.001\n",
            "Epoch: 500, Train Loss: 26796043010048.000000, Val Loss: 23884904529920.000000\n",
            "Epoch: 1000, Train Loss: 26796034621440.000000, Val Loss: 23884894044160.000000\n",
            "Epoch: 1500, Train Loss: 26796024135680.000000, Val Loss: 23884883558400.000000\n",
            "Epoch: 2000, Train Loss: 26796017844224.000000, Val Loss: 23884879364096.000000\n",
            "Epoch: 2500, Train Loss: 26796005261312.000000, Val Loss: 23884873072640.000000\n",
            "Epoch: 3000, Train Loss: 26795998969856.000000, Val Loss: 23884864684032.000000\n",
            "Epoch: 3500, Train Loss: 26795988484096.000000, Val Loss: 23884860489728.000000\n",
            "Epoch: 4000, Train Loss: 26795977998336.000000, Val Loss: 23884847906816.000000\n",
            "Epoch: 4500, Train Loss: 26795967512576.000000, Val Loss: 23884841615360.000000\n",
            "Epoch: 5000, Train Loss: 26795961221120.000000, Val Loss: 23884835323904.000000\n",
            "Adam Optimizer - Learning Rate: 0.0001\n",
            "Epoch: 500, Train Loss: 26796051398656.000000, Val Loss: 23884908724224.000000\n",
            "Epoch: 1000, Train Loss: 26796051398656.000000, Val Loss: 23884906627072.000000\n",
            "Epoch: 1500, Train Loss: 26796051398656.000000, Val Loss: 23884906627072.000000\n",
            "Epoch: 2000, Train Loss: 26796049301504.000000, Val Loss: 23884906627072.000000\n",
            "Epoch: 2500, Train Loss: 26796049301504.000000, Val Loss: 23884906627072.000000\n",
            "Epoch: 3000, Train Loss: 26796049301504.000000, Val Loss: 23884904529920.000000\n",
            "Epoch: 3500, Train Loss: 26796049301504.000000, Val Loss: 23884904529920.000000\n",
            "Epoch: 4000, Train Loss: 26796045107200.000000, Val Loss: 23884904529920.000000\n",
            "Epoch: 4500, Train Loss: 26796045107200.000000, Val Loss: 23884904529920.000000\n",
            "Epoch: 5000, Train Loss: 26796043010048.000000, Val Loss: 23884904529920.000000\n",
            "SGD Optimizer - Learning Rate: 0.1\n",
            "Epoch: 500, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 1000, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 1500, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 2000, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 2500, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 3000, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 3500, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 4000, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 4500, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "Epoch: 5000, Train Loss: 1539478913024.000000, Val Loss: 1546246684672.000000\n",
            "SGD Optimizer - Learning Rate: 0.01\n",
            "Epoch: 500, Train Loss: 1539481010176.000000, Val Loss: 1544927313920.000000\n",
            "Epoch: 1000, Train Loss: 1539478913024.000000, Val Loss: 1546238689280.000000\n",
            "Epoch: 1500, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 2000, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 2500, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 3000, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 3500, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 4000, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 4500, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "Epoch: 5000, Train Loss: 1539478913024.000000, Val Loss: 1546241179648.000000\n",
            "SGD Optimizer - Learning Rate: 0.001\n",
            "Epoch: 500, Train Loss: 4750372765696.000000, Val Loss: 3950534459392.000000\n",
            "Epoch: 1000, Train Loss: 1982677516288.000000, Val Loss: 1645125828608.000000\n",
            "Epoch: 1500, Train Loss: 1603834609664.000000, Val Loss: 1454748073984.000000\n",
            "Epoch: 2000, Train Loss: 1549595443200.000000, Val Loss: 1483718524928.000000\n",
            "Epoch: 2500, Train Loss: 1541285085184.000000, Val Loss: 1513059254272.000000\n",
            "Epoch: 3000, Train Loss: 1539858497536.000000, Val Loss: 1529204572160.000000\n",
            "Epoch: 3500, Train Loss: 1539571974144.000000, Val Loss: 1537411252224.000000\n",
            "Epoch: 4000, Train Loss: 1539504472064.000000, Val Loss: 1541587599360.000000\n",
            "Epoch: 4500, Train Loss: 1539486384128.000000, Val Loss: 1543752515584.000000\n",
            "Epoch: 5000, Train Loss: 1539481010176.000000, Val Loss: 1544888647680.000000\n",
            "SGD Optimizer - Learning Rate: 0.0001\n",
            "Epoch: 500, Train Loss: 21920368033792.000000, Val Loss: 19487539593216.000000\n",
            "Epoch: 1000, Train Loss: 18023618445312.000000, Val Loss: 15959931748352.000000\n",
            "Epoch: 1500, Train Loss: 14902825582592.000000, Val Loss: 13127800324096.000000\n",
            "Epoch: 2000, Train Loss: 12394195582976.000000, Val Loss: 10848605044736.000000\n",
            "Epoch: 2500, Train Loss: 10371246063616.000000, Val Loss: 9011085705216.000000\n",
            "Epoch: 3000, Train Loss: 8735515213824.000000, Val Loss: 7527736016896.000000\n",
            "Epoch: 3500, Train Loss: 7409855627264.000000, Val Loss: 6329330565120.000000\n",
            "Epoch: 4000, Train Loss: 6333388554240.000000, Val Loss: 5360753573888.000000\n",
            "Epoch: 4500, Train Loss: 5457841225728.000000, Val Loss: 4577943879680.000000\n",
            "Epoch: 5000, Train Loss: 4744709406720.000000, Val Loss: 3945510731776.000000\n",
            "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
            "0  13300000  7420         4          2        3         1          0   \n",
            "1  12250000  8960         4          4        4         1          0   \n",
            "2  12250000  9960         3          2        2         1          0   \n",
            "3  12215000  7500         4          2        2         1          0   \n",
            "4  11410000  7420         4          1        2         1          1   \n",
            "\n",
            "   basement  hotwaterheating  airconditioning  parking prefarea  \\\n",
            "0         0                0                1        2      yes   \n",
            "1         0                0                1        3       no   \n",
            "2         1                0                0        2      yes   \n",
            "3         1                0                1        3      yes   \n",
            "4         1                0                1        2       no   \n",
            "\n",
            "  furnishingstatus  \n",
            "0        furnished  \n",
            "1        furnished  \n",
            "2   semi-furnished  \n",
            "3        furnished  \n",
            "4        furnished  \n",
            "Adam Optimizer - Learning Rate: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([109])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 500, Train Loss: 26795512430592.000000, Val Loss: 23884719980544.000000\n",
            "Epoch: 1000, Train Loss: 26794962976768.000000, Val Loss: 23884524945408.000000\n",
            "Epoch: 1500, Train Loss: 26794413522944.000000, Val Loss: 23884332007424.000000\n",
            "Epoch: 2000, Train Loss: 26793859874816.000000, Val Loss: 23884136972288.000000\n",
            "Epoch: 2500, Train Loss: 26793308323840.000000, Val Loss: 23883941937152.000000\n",
            "Epoch: 3000, Train Loss: 26792760967168.000000, Val Loss: 23883748999168.000000\n",
            "Epoch: 3500, Train Loss: 26792209416192.000000, Val Loss: 23883551866880.000000\n",
            "Epoch: 4000, Train Loss: 26791662059520.000000, Val Loss: 23883356831744.000000\n",
            "Epoch: 4500, Train Loss: 26791106314240.000000, Val Loss: 23883163893760.000000\n",
            "Epoch: 5000, Train Loss: 26790561054720.000000, Val Loss: 23882970955776.000000\n",
            "Adam Optimizer - Learning Rate: 0.01\n",
            "Epoch: 500, Train Loss: 26796007358464.000000, Val Loss: 23884896141312.000000\n",
            "Epoch: 1000, Train Loss: 26795950735360.000000, Val Loss: 23884877266944.000000\n",
            "Epoch: 1500, Train Loss: 26795898306560.000000, Val Loss: 23884856295424.000000\n",
            "Epoch: 2000, Train Loss: 26795841683456.000000, Val Loss: 23884837421056.000000\n",
            "Epoch: 2500, Train Loss: 26795782963200.000000, Val Loss: 23884816449536.000000\n",
            "Epoch: 3000, Train Loss: 26795732631552.000000, Val Loss: 23884797575168.000000\n",
            "Epoch: 3500, Train Loss: 26795676008448.000000, Val Loss: 23884778700800.000000\n",
            "Epoch: 4000, Train Loss: 26795621482496.000000, Val Loss: 23884759826432.000000\n",
            "Epoch: 4500, Train Loss: 26795564859392.000000, Val Loss: 23884738854912.000000\n",
            "Epoch: 5000, Train Loss: 26795512430592.000000, Val Loss: 23884719980544.000000\n",
            "Adam Optimizer - Learning Rate: 0.001\n",
            "Epoch: 500, Train Loss: 26796057690112.000000, Val Loss: 23884910821376.000000\n",
            "Epoch: 1000, Train Loss: 26796049301504.000000, Val Loss: 23884910821376.000000\n",
            "Epoch: 1500, Train Loss: 26796043010048.000000, Val Loss: 23884910821376.000000\n",
            "Epoch: 2000, Train Loss: 26796038815744.000000, Val Loss: 23884908724224.000000\n",
            "Epoch: 2500, Train Loss: 26796032524288.000000, Val Loss: 23884906627072.000000\n",
            "Epoch: 3000, Train Loss: 26796030427136.000000, Val Loss: 23884902432768.000000\n",
            "Epoch: 3500, Train Loss: 26796024135680.000000, Val Loss: 23884900335616.000000\n",
            "Epoch: 4000, Train Loss: 26796017844224.000000, Val Loss: 23884898238464.000000\n",
            "Epoch: 4500, Train Loss: 26796015747072.000000, Val Loss: 23884896141312.000000\n",
            "Epoch: 5000, Train Loss: 26796007358464.000000, Val Loss: 23884896141312.000000\n",
            "Adam Optimizer - Learning Rate: 0.0001\n",
            "Epoch: 500, Train Loss: 26796061884416.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 1000, Train Loss: 26796061884416.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 1500, Train Loss: 26796061884416.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 2000, Train Loss: 26796057690112.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 2500, Train Loss: 26796057690112.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 3000, Train Loss: 26796057690112.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 3500, Train Loss: 26796057690112.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 4000, Train Loss: 26796055592960.000000, Val Loss: 23884915015680.000000\n",
            "Epoch: 4500, Train Loss: 26796055592960.000000, Val Loss: 23884910821376.000000\n",
            "Epoch: 5000, Train Loss: 26796055592960.000000, Val Loss: 23884910821376.000000\n",
            "SGD Optimizer - Learning Rate: 0.1\n",
            "Epoch: 500, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 1000, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 1500, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 2000, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 2500, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 3000, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 3500, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 4000, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 4500, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "Epoch: 5000, Train Loss: 3593712435200.000000, Val Loss: 3098350977024.000000\n",
            "SGD Optimizer - Learning Rate: 0.01\n",
            "Epoch: 500, Train Loss: 3593713483776.000000, Val Loss: 3098123173888.000000\n",
            "Epoch: 1000, Train Loss: 3593711910912.000000, Val Loss: 3098344423424.000000\n",
            "Epoch: 1500, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 2000, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 2500, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 3000, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 3500, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 4000, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 4500, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "Epoch: 5000, Train Loss: 3593712435200.000000, Val Loss: 3098345472000.000000\n",
            "SGD Optimizer - Learning Rate: 0.001\n",
            "Epoch: 500, Train Loss: 6739365199872.000000, Val Loss: 5443047391232.000000\n",
            "Epoch: 1000, Train Loss: 4023950245888.000000, Val Loss: 3220069154816.000000\n",
            "Epoch: 1500, Train Loss: 3653662146560.000000, Val Loss: 3037740662784.000000\n",
            "Epoch: 2000, Train Loss: 3602345623552.000000, Val Loss: 3060004552704.000000\n",
            "Epoch: 2500, Train Loss: 3595035213824.000000, Val Loss: 3081303490560.000000\n",
            "Epoch: 3000, Train Loss: 3593938665472.000000, Val Loss: 3091306119168.000000\n",
            "Epoch: 3500, Train Loss: 3593758310400.000000, Val Loss: 3095457955840.000000\n",
            "Epoch: 4000, Train Loss: 3593723969536.000000, Val Loss: 3097137512448.000000\n",
            "Epoch: 4500, Train Loss: 3593716105216.000000, Val Loss: 3097823543296.000000\n",
            "Epoch: 5000, Train Loss: 3593713745920.000000, Val Loss: 3098103250944.000000\n",
            "SGD Optimizer - Learning Rate: 0.0001\n",
            "Epoch: 500, Train Loss: 22586379468800.000000, Val Loss: 19963247067136.000000\n",
            "Epoch: 1000, Train Loss: 19136316964864.000000, Val Loss: 16757746040832.000000\n",
            "Epoch: 1500, Train Loss: 16314583220224.000000, Val Loss: 14144901218304.000000\n",
            "Epoch: 2000, Train Loss: 14006419980288.000000, Val Loss: 12016533110784.000000\n",
            "Epoch: 2500, Train Loss: 12118093987840.000000, Val Loss: 10284088426496.000000\n",
            "Epoch: 3000, Train Loss: 10573040320512.000000, Val Loss: 8875103223808.000000\n",
            "Epoch: 3500, Train Loss: 9308701982720.000000, Val Loss: 7730271092736.000000\n",
            "Epoch: 4000, Train Loss: 8273955127296.000000, Val Loss: 6801071276032.000000\n",
            "Epoch: 4500, Train Loss: 7427001417728.000000, Val Loss: 6047804686336.000000\n",
            "Epoch: 5000, Train Loss: 6733701840896.000000, Val Loss: 5438023139328.000000\n"
          ]
        }
      ]
    }
  ]
}